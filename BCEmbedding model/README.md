---
license: apache-2.0
pipeline_tag: feature-extraction
tags:
- sentence-transformers
- feature-extraction
- sentence-similarity
- transformers
language:
- en
- zh
---
<!--
 * @Description: 
 * @Author: shenlei
 * @Date: 2023-12-19 10:31:41
 * @LastEditTime: 2024-01-09 23:52:00
 * @LastEditors: shenlei
-->
<h1 align="center">BCEmbedding: Bilingual and Crosslingual Embedding for RAG</h1>

<p align="center">
  <a href="https://github.com/netease-youdao/BCEmbedding/blob/master/LICENSE">
    <img src="https://img.shields.io/badge/license-Apache--2.0-yellow">
  </a>
  <a href="https://twitter.com/YDopensource">
    <img src="https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}">
  </a>
</p>

æœ€æ–°ã€æœ€è¯¦ç»†çš„bce-embedding-base_v1ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç§»æ­¥ï¼ˆThe latest "Updates" should be checked inï¼‰ï¼š
  
<p align="left">
  <a href="https://github.com/netease-youdao/BCEmbedding">GitHub</a>
</p>

## ä¸»è¦ç‰¹ç‚¹(Key Features)ï¼š
- ä¸­è‹±åŒè¯­ï¼Œä»¥åŠä¸­è‹±è·¨è¯­ç§èƒ½åŠ›(Bilingual and Crosslingual capability in English and Chinese)ï¼›
- RAGä¼˜åŒ–ï¼Œé€‚é…æ›´å¤šçœŸå®ä¸šåŠ¡åœºæ™¯(RAG adaptation for more domains, including Education, Law, Finance, Medical, Literature, FAQ, Textbook, Wikipedia, etc.)ï¼›
- æ–¹ä¾¿é›†æˆè¿›langchainå’Œllamaindex(Easy integrations for langchain and llamaindex in <a href="https://github.com/netease-youdao/BCEmbedding">BCEmbedding</a>)ã€‚
- `EmbeddingModel`ä¸éœ€è¦â€œç²¾å¿ƒè®¾è®¡â€instructionï¼Œå°½å¯èƒ½å¬å›æœ‰ç”¨ç‰‡æ®µã€‚ (No need for "instruction")
- **æœ€ä½³å®è·µï¼ˆBest practiceï¼‰** ï¼šembeddingå¬å›top50-100ç‰‡æ®µï¼Œrerankerå¯¹è¿™50-100ç‰‡æ®µç²¾æ’ï¼Œæœ€åå–top5-10ç‰‡æ®µã€‚ï¼ˆ1. Get top 50-100 passages with [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1) for "`recall`"ï¼›    2. Rerank passages with [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1) and get top 5-10 for "`precision`" finally. ï¼‰

## News:
- `BCEmbedding`æŠ€æœ¯åšå®¢ï¼ˆ **Technical Blog** ï¼‰: [ä¸ºRAGè€Œç”Ÿ-BCEmbeddingæŠ€æœ¯æŠ¥å‘Š](https://zhuanlan.zhihu.com/p/681370855)
- Related link for **RerankerModel** : [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1)

## Third-party Examples:
- RAG applications: [QAnything](https://github.com/netease-youdao/qanything), [HuixiangDou](https://github.com/InternLM/HuixiangDou), [ChatPDF](https://github.com/shibing624/ChatPDF).
- Efficient inference framework: [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp), [Xinference](https://github.com/xorbitsai/inference), [mindnlp (Huawei GPU, åä¸ºGPU)](https://github.com/mindspore-lab/mindnlp/tree/master/llm/inference/bce).

![image/jpeg](assets/rag_eval_multiple_domains_summary.jpg)
![image/jpeg](assets/Wechat.jpg)

-----------------------------------------
<details open="open">
<summary>Click to Open Contents</summary>

- <a href="#-bilingual-and-crosslingual-superiority" target="_Self">ğŸŒ Bilingual and Crosslingual Superiority</a>
- <a href="#-key-features" target="_Self">ğŸ’¡ Key Features</a>
- <a href="#-latest-updates" target="_Self">ğŸš€ Latest Updates</a>
- <a href="#-model-list" target="_Self">ğŸ Model List</a>
- <a href="#-manual" target="_Self">ğŸ“– Manual</a>
  - <a href="#installation" target="_Self">Installation</a>
  - <a href="#quick-start" target="_Self">Quick Start (`transformers`, `sentence-transformers`)</a>
  - <a href="#integrations-for-rag-frameworks" target="_Self">Integrations for RAG Frameworks (`langchain`, `llama_index`)</a>
- <a href="#%EF%B8%8F-evaluation" target="_Self">âš™ï¸ Evaluation</a>
  - <a href="#evaluate-semantic-representation-by-mteb" target="_Self">Evaluate Semantic Representation by MTEB</a>
  - <a href="#evaluate-rag-by-llamaindex" target="_Self">Evaluate RAG by LlamaIndex</a>
- <a href="#-leaderboard" target="_Self">ğŸ“ˆ Leaderboard</a>
  - <a href="#semantic-representation-evaluations-in-mteb" target="_Self">Semantic Representation Evaluations in MTEB</a>
  - <a href="#rag-evaluations-in-llamaindex" target="_Self">RAG Evaluations in LlamaIndex</a>
- <a href="#-youdaos-bcembedding-api" target="_Self">ğŸ›  Youdao's BCEmbedding API</a>
- <a href="#-wechat-group" target="_Self">ğŸ§² WeChat Group</a>
- <a href="#%EF%B8%8F-citation" target="_Self">âœï¸ Citation</a>
- <a href="#-license" target="_Self">ğŸ” License</a>
- <a href="#-related-links" target="_Self">ğŸ”— Related Links</a>

</details>
<br>

**B**ilingual and **C**rosslingual **Embedding** (`BCEmbedding`), developed by NetEase Youdao, encompasses `EmbeddingModel` and `RerankerModel`. The `EmbeddingModel` specializes in generating semantic vectors, playing a crucial role in semantic search and question-answering, and the `RerankerModel` excels at refining search results and ranking tasks. 

`BCEmbedding` serves as the cornerstone of Youdao's Retrieval Augmented Generation (RAG) implmentation, notably [QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)], an open-source implementation widely integrated in various Youdao products like [Youdao Speed Reading](https://read.youdao.com/#/home) and [Youdao Translation](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation). 

Distinguished for its bilingual and crosslingual proficiency, `BCEmbedding` excels in bridging Chinese and English linguistic gaps, which achieves
- **A high performence on <a href="#semantic-representation-evaluations-in-mteb">Semantic Representation Evaluations in MTEB</a>**;
- **A new benchmark in the realm of <a href="#rag-evaluations-in-llamaindex">RAG Evaluations in LlamaIndex</a>**.

  `BCEmbedding`æ˜¯ç”±ç½‘æ˜“æœ‰é“å¼€å‘çš„åŒè¯­å’Œè·¨è¯­ç§è¯­ä¹‰è¡¨å¾ç®—æ³•æ¨¡å‹åº“ï¼Œå…¶ä¸­åŒ…å«`EmbeddingModel`å’Œ`RerankerModel`ä¸¤ç±»åŸºç¡€æ¨¡å‹ã€‚`EmbeddingModel`ä¸“é—¨ç”¨äºç”Ÿæˆè¯­ä¹‰å‘é‡ï¼Œåœ¨è¯­ä¹‰æœç´¢å’Œé—®ç­”ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œè€Œ`RerankerModel`æ“…é•¿ä¼˜åŒ–è¯­ä¹‰æœç´¢ç»“æœå’Œè¯­ä¹‰ç›¸å…³é¡ºåºç²¾æ’ã€‚
  
  `BCEmbedding`ä½œä¸ºæœ‰é“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆå¼åº”ç”¨ï¼ˆRAGï¼‰çš„åŸºçŸ³ï¼Œç‰¹åˆ«æ˜¯åœ¨[QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)]ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚QAnythingä½œä¸ºä¸€ä¸ªç½‘æ˜“æœ‰é“å¼€æºé¡¹ç›®ï¼Œåœ¨æœ‰é“è®¸å¤šäº§å“ä¸­æœ‰å¾ˆå¥½çš„åº”ç”¨å®è·µï¼Œæ¯”å¦‚[æœ‰é“é€Ÿè¯»](https://read.youdao.com/#/home)å’Œ[æœ‰é“ç¿»è¯‘](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation)
  
  `BCEmbedding`ä»¥å…¶å‡ºè‰²çš„åŒè¯­å’Œè·¨è¯­ç§èƒ½åŠ›è€Œè‘—ç§°ï¼Œåœ¨è¯­ä¹‰æ£€ç´¢ä¸­æ¶ˆé™¤ä¸­è‹±è¯­è¨€ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œå®ç°ï¼š
  - **å¼ºå¤§çš„åŒè¯­å’Œè·¨è¯­ç§è¯­ä¹‰è¡¨å¾èƒ½åŠ›ã€<a href="#semantic-representation-evaluations-in-mteb">åŸºäºMTEBçš„è¯­ä¹‰è¡¨å¾è¯„æµ‹æŒ‡æ ‡</a>ã€‘ã€‚**
  - **åŸºäºLlamaIndexçš„RAGè¯„æµ‹ï¼Œè¡¨ç°SOTAã€<a href="#rag-evaluations-in-llamaindex">åŸºäºLlamaIndexçš„RAGè¯„æµ‹æŒ‡æ ‡</a>ã€‘ã€‚**

## ğŸŒ Bilingual and Crosslingual Superiority

Existing embedding models often encounter performance challenges in bilingual and crosslingual scenarios, particularly in Chinese, English and their crosslingual tasks. `BCEmbedding`, leveraging the strength of Youdao's translation engine, excels in delivering superior performance across monolingual, bilingual, and crosslingual settings.

`EmbeddingModel` supports ***Chinese (ch) and English (en)*** (more languages support will come soon), while `RerankerModel` supports ***Chinese (ch), English (en), Japanese (ja) and Korean (ko)***.

  ç°æœ‰çš„å•ä¸ªè¯­ä¹‰è¡¨å¾æ¨¡å‹åœ¨åŒè¯­å’Œè·¨è¯­ç§åœºæ™¯ä¸­å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡ã€è‹±æ–‡åŠå…¶è·¨è¯­ç§ä»»åŠ¡ä¸­ã€‚`BCEmbedding`å……åˆ†åˆ©ç”¨æœ‰é“ç¿»è¯‘å¼•æ“çš„ä¼˜åŠ¿ï¼Œå®ç°åªéœ€ä¸€ä¸ªæ¨¡å‹å°±å¯ä»¥åœ¨å•è¯­ã€åŒè¯­å’Œè·¨è¯­ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚
  
  `EmbeddingModel`æ”¯æŒ***ä¸­æ–‡å’Œè‹±æ–‡***ï¼ˆä¹‹åä¼šæ”¯æŒæ›´å¤šè¯­ç§ï¼‰ï¼›`RerankerModel`æ”¯æŒ***ä¸­æ–‡ï¼Œè‹±æ–‡ï¼Œæ—¥æ–‡å’ŒéŸ©æ–‡***ã€‚

## ğŸ’¡ Key Features

- **Bilingual and Crosslingual Proficiency**: Powered by Youdao's translation engine, excelling in Chinese, English and their crosslingual retrieval task, with upcoming support for additional languages.

- **RAG-Optimized**: Tailored for diverse RAG tasks including **translation, summarization, and question answering**, ensuring accurate **query understanding**. See <a href=#rag-evaluations-in-llamaindex>RAG Evaluations in LlamaIndex</a>.

- **Efficient and Precise Retrieval**: Dual-encoder for efficient retrieval of `EmbeddingModel` in first stage, and cross-encoder of `RerankerModel` for enhanced precision and deeper semantic analysis in second stage.

- **Broad Domain Adaptability**: Trained on diverse datasets for superior performance across various fields.

- **User-Friendly Design**: Instruction-free, versatile use for multiple tasks without specifying query instruction for each task.

- **Meaningful Reranking Scores**: `RerankerModel` provides relevant scores to improve result quality and optimize large language model performance.

- **Proven in Production**: Successfully implemented and validated in Youdao's products.

  - **åŒè¯­å’Œè·¨è¯­ç§èƒ½åŠ›**ï¼šåŸºäºæœ‰é“ç¿»è¯‘å¼•æ“çš„å¼ºå¤§èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„`BCEmbedding`å…·å¤‡å¼ºå¤§çš„ä¸­è‹±åŒè¯­å’Œè·¨è¯­ç§è¯­ä¹‰è¡¨å¾èƒ½åŠ›ã€‚
  
  - **RAGé€‚é…**ï¼šé¢å‘RAGåšäº†é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œå¯ä»¥é€‚é…å¤§å¤šæ•°ç›¸å…³ä»»åŠ¡ï¼Œæ¯”å¦‚**ç¿»è¯‘ï¼Œæ‘˜è¦ï¼Œé—®ç­”**ç­‰ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹**é—®é¢˜ç†è§£**ï¼ˆquery understandingï¼‰ä¹Ÿåšäº†é’ˆå¯¹ä¼˜åŒ–ï¼Œè¯¦è§ <a href="#rag-evaluations-in-llamaindex">åŸºäºLlamaIndexçš„RAGè¯„æµ‹æŒ‡æ ‡</a>ã€‚
  
  - **é«˜æ•ˆä¸”ç²¾ç¡®çš„è¯­ä¹‰æ£€ç´¢**ï¼š`EmbeddingModel`é‡‡ç”¨åŒç¼–ç å™¨ï¼Œå¯ä»¥åœ¨ç¬¬ä¸€é˜¶æ®µå®ç°é«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢ã€‚`RerankerModel`é‡‡ç”¨äº¤å‰ç¼–ç å™¨ï¼Œå¯ä»¥åœ¨ç¬¬äºŒé˜¶æ®µå®ç°æ›´é«˜ç²¾åº¦çš„è¯­ä¹‰é¡ºåºç²¾æ’ã€‚
  
  - **æ›´å¥½çš„é¢†åŸŸæ³›åŒ–æ€§**ï¼šä¸ºäº†åœ¨æ›´å¤šåœºæ™¯å®ç°æ›´å¥½çš„æ•ˆæœï¼Œæˆ‘ä»¬æ”¶é›†äº†å¤šç§å¤šæ ·çš„é¢†åŸŸæ•°æ®ã€‚
  
  - **ç”¨æˆ·å‹å¥½**ï¼šè¯­ä¹‰æ£€ç´¢æ—¶ä¸éœ€è¦ç‰¹æ®ŠæŒ‡ä»¤å‰ç¼€ã€‚ä¹Ÿå°±æ˜¯ï¼Œä½ ä¸éœ€è¦ä¸ºå„ç§ä»»åŠ¡ç»å°½è„‘æ±è®¾è®¡æŒ‡ä»¤å‰ç¼€ã€‚
  
  - **æœ‰æ„ä¹‰çš„é‡æ’åºåˆ†æ•°**ï¼š`RerankerModel`å¯ä»¥æä¾›æœ‰æ„ä¹‰çš„è¯­ä¹‰ç›¸å…³æ€§åˆ†æ•°ï¼ˆä¸ä»…ä»…æ˜¯æ’åºï¼‰ï¼Œå¯ä»¥ç”¨äºè¿‡æ»¤æ— æ„ä¹‰æ–‡æœ¬ç‰‡æ®µï¼Œæé«˜å¤§æ¨¡å‹ç”Ÿæˆæ•ˆæœã€‚
  
  - **äº§å“åŒ–æ£€éªŒ**ï¼š`BCEmbedding`å·²ç»è¢«æœ‰é“ä¼—å¤šçœŸå®äº§å“æ£€éªŒã€‚

## ğŸš€ Latest Updates

- ***2024-01-03***: **Model Releases** - [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1) and [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1) are available.
- ***2024-01-03***: **Eval Datasets** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - Evaluate the performence of RAG, using [LlamaIndex](https://github.com/run-llama/llama_index).
- ***2024-01-03***: **Eval Datasets** [[Details](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)] - Evaluate the performence of crosslingual semantic representation, using [MTEB](https://github.com/embeddings-benchmark/mteb).

  - ***2024-01-03***: **æ¨¡å‹å‘å¸ƒ** - [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1)å’Œ[bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1)å·²å‘å¸ƒ.
  - ***2024-01-03***: **RAGè¯„æµ‹æ•°æ®** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - åŸºäº[LlamaIndex](https://github.com/run-llama/llama_index)çš„RAGè¯„æµ‹æ•°æ®å·²å‘å¸ƒã€‚
  - ***2024-01-03***: **è·¨è¯­ç§è¯­ä¹‰è¡¨å¾è¯„æµ‹æ•°æ®** [[è¯¦æƒ…](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)] - åŸºäº[MTEB](https://github.com/embeddings-benchmark/mteb)çš„è·¨è¯­ç§è¯„æµ‹æ•°æ®å·²å‘å¸ƒ.

## ğŸ Model List

| Model Name | Model Type | Languages | Parameters | Weights |  
|:-------------------------------|:--------:|:--------:|:--------:|:--------:|  
| bce-embedding-base_v1 | `EmbeddingModel` | ch, en | 279M | [download](https://huggingface.co/maidalun1020/bce-embedding-base_v1) |  
| bce-reranker-base_v1 | `RerankerModel` | ch, en, ja, ko | 279M | [download](https://huggingface.co/maidalun1020/bce-reranker-base_v1) |  

## ğŸ“– Manual

### Installation

First, create a conda environment and activate it.

```bash
conda create --name bce python=3.10 -y
conda activate bce
```

Then install `BCEmbedding` for minimal installation:

```bash
pip install BCEmbedding==0.1.1
```

Or install from source:

```bash
git clone git@github.com:netease-youdao/BCEmbedding.git
cd BCEmbedding
pip install -v -e .
```

### Quick Start

#### 1. Based on `BCEmbedding`

Use `EmbeddingModel`, and `cls` [pooler](./BCEmbedding/models/embedding.py#L24) is default.

```python
from BCEmbedding import EmbeddingModel

# list of sentences
sentences = ['sentence_0', 'sentence_1', ...]

# init embedding model
model = EmbeddingModel(model_name_or_path="maidalun1020/bce-embedding-base_v1")

# extract embeddings
embeddings = model.encode(sentences)
```

Use `RerankerModel` to calculate relevant scores and rerank:

```python
from BCEmbedding import RerankerModel

# your query and corresponding passages
query = 'input_query'
passages = ['passage_0', 'passage_1', ...]

# construct sentence pairs
sentence_pairs = [[query, passage] for passage in passages]

# init reranker model
model = RerankerModel(model_name_or_path="maidalun1020/bce-reranker-base_v1")

# method 0: calculate scores of sentence pairs
scores = model.compute_score(sentence_pairs)

# method 1: rerank passages
rerank_results = model.rerank(query, passages)
```

NOTE:

- In [`RerankerModel.rerank`](./BCEmbedding/models/reranker.py#L137) method, we provide an advanced preproccess that we use in production for making `sentence_pairs`, when "passages" are very long.

#### 2. Based on `transformers`

For `EmbeddingModel`:

```python
from transformers import AutoModel, AutoTokenizer

# list of sentences
sentences = ['sentence_0', 'sentence_1', ...]

# init model and tokenizer
tokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-embedding-base_v1')
model = AutoModel.from_pretrained('maidalun1020/bce-embedding-base_v1')

device = 'cuda'  # if no GPU, set "cpu"
model.to(device)

# get inputs
inputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors="pt")
inputs_on_device = {k: v.to(self.device) for k, v in inputs.items()}

# get embeddings
outputs = model(**inputs_on_device, return_dict=True)
embeddings = outputs.last_hidden_state[:, 0]  # cls pooler
embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  # normalize
```

For `RerankerModel`:

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# init model and tokenizer
tokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-reranker-base_v1')
model = AutoModelForSequenceClassification.from_pretrained('maidalun1020/bce-reranker-base_v1')

device = 'cuda'  # if no GPU, set "cpu"
model.to(device)

# get inputs
inputs = tokenizer(sentence_pairs, padding=True, truncation=True, max_length=512, return_tensors="pt")
inputs_on_device = {k: v.to(device) for k, v in inputs.items()}

# calculate scores
scores = model(**inputs_on_device, return_dict=True).logits.view(-1,).float()
scores = torch.sigmoid(scores)
```

#### 3. Based on `sentence_transformers`

For `EmbeddingModel`:

```python
from sentence_transformers import SentenceTransformer

# list of sentences
sentences = ['sentence_0', 'sentence_1', ...]

# init embedding model
## New update for sentence-trnasformers. So clean up your "`SENTENCE_TRANSFORMERS_HOME`/maidalun1020_bce-embedding-base_v1" or "ï½/.cache/torch/sentence_transformers/maidalun1020_bce-embedding-base_v1" first for downloading new version.
model = SentenceTransformer("maidalun1020/bce-embedding-base_v1")

# extract embeddings
embeddings = model.encode(sentences, normalize_embeddings=True)
```

For `RerankerModel`:

```python
from sentence_transformers import CrossEncoder

# init reranker model
model = CrossEncoder('maidalun1020/bce-reranker-base_v1', max_length=512)

# calculate scores of sentence pairs
scores = model.predict(sentence_pairs)
```

### Integrations for RAG Frameworks

#### 1. Used in `langchain`

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy

query = 'apples'
passages = [
        'I like apples', 
        'I like oranges', 
        'Apples and oranges are fruits'
    ]
  
# init embedding model
model_name = 'maidalun1020/bce-embedding-base_v1'
model_kwargs = {'device': 'cuda'}
encode_kwargs = {'batch_size': 64, 'normalize_embeddings': True, 'show_progress_bar': False}

embed_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
  )

# example #1. extract embeddings
query_embedding = embed_model.embed_query(query)
passages_embeddings = embed_model.embed_documents(passages)

# example #2. langchain retriever example
faiss_vectorstore = FAISS.from_texts(passages, embed_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)

retriever = faiss_vectorstore.as_retriever(search_type="similarity", search_kwargs={"score_threshold": 0.5, "k": 3})

related_passages = retriever.get_relevant_documents(query)
```

#### 2. Used in `llama_index`

```python
from llama_index.embeddings import HuggingFaceEmbedding
from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader
from llama_index.node_parser import SimpleNodeParser
from llama_index.llms import OpenAI

query = 'apples'
passages = [
        'I like apples', 
        'I like oranges', 
        'Apples and oranges are fruits'
    ]

# init embedding model
model_args = {'model_name': 'maidalun1020/bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 64, 'device': 'cuda'}
embed_model = HuggingFaceEmbedding(**model_args)

# example #1. extract embeddings
query_embedding = embed_model.get_query_embedding(query)
passages_embeddings = embed_model.get_text_embedding_batch(passages)

# example #2. rag example
llm = OpenAI(model='gpt-3.5-turbo-0613', api_key=os.environ.get('OPENAI_API_KEY'), api_base=os.environ.get('OPENAI_BASE_URL'))
service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)

documents = SimpleDirectoryReader(input_files=["BCEmbedding/tools/eval_rag/eval_pdfs/Comp_en_llama2.pdf"]).load_data()
node_parser = SimpleNodeParser.from_defaults(chunk_size=512)
nodes = node_parser.get_nodes_from_documents(documents[0:36])
index = VectorStoreIndex(nodes, service_context=service_context)
query_engine = index.as_query_engine()
response = query_engine.query("What is llama?")
```


## âš™ï¸ Evaluation

### Evaluate Semantic Representation by MTEB

We provide evaluateion tools for `embedding` and `reranker` models, based on [MTEB](https://github.com/embeddings-benchmark/mteb) and [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB).

  æˆ‘ä»¬åŸºäº[MTEB](https://github.com/embeddings-benchmark/mteb)å’Œ[C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)ï¼Œæä¾›`embedding`å’Œ`reranker`æ¨¡å‹çš„è¯­ä¹‰è¡¨å¾è¯„æµ‹å·¥å…·ã€‚

#### 1. Embedding Models

Just run following cmd to evaluate `your_embedding_model` (e.g. `maidalun1020/bce-embedding-base_v1`) in **bilingual and crosslingual settings** (e.g. `["en", "zh", "en-zh", "zh-en"]`).

  è¿è¡Œä¸‹é¢å‘½ä»¤è¯„æµ‹`your_embedding_model`ï¼ˆæ¯”å¦‚ï¼Œ`maidalun1020/bce-embedding-base_v1`ï¼‰ã€‚è¯„æµ‹ä»»åŠ¡å°†ä¼šåœ¨**åŒè¯­å’Œè·¨è¯­ç§**ï¼ˆæ¯”å¦‚ï¼Œ`["en", "zh", "en-zh", "zh-en"]`ï¼‰æ¨¡å¼ä¸‹è¯„æµ‹ï¼š

```bash
python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path maidalun1020/bce-embedding-base_v1 --pooler cls
```

The total evaluation tasks contain ***114 datastes*** of **"Retrieval", "STS", "PairClassification", "Classification", "Reranking" and "Clustering"**.

  è¯„æµ‹åŒ…å« **"Retrieval"ï¼Œ "STS"ï¼Œ "PairClassification"ï¼Œ "Classification"ï¼Œ "Reranking"å’Œ"Clustering"** è¿™å…­å¤§ç±»ä»»åŠ¡çš„ ***114ä¸ªæ•°æ®é›†***ã€‚

***NOTE:***
- **All models are evaluated in their recommended pooling method (`pooler`)**.
  - `mean` pooler: "jina-embeddings-v2-base-en", "m3e-base", "m3e-large", "e5-large-v2", "multilingual-e5-base", "multilingual-e5-large" and "gte-large".
  - `cls` pooler: Other models.
- "jina-embeddings-v2-base-en" model should be loaded with `trust_remote_code`.

```bash
python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path {moka-ai/m3e-base | moka-ai/m3e-large} --pooler mean

python BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path jinaai/jina-embeddings-v2-base-en --pooler mean --trust_remote_code
```

  ***æ³¨æ„ï¼š***
  - æ‰€æœ‰æ¨¡å‹çš„è¯„æµ‹é‡‡ç”¨å„è‡ªæ¨èçš„`pooler`ã€‚"jina-embeddings-v2-base-en", "m3e-base", "m3e-large", "e5-large-v2", "multilingual-e5-base", "multilingual-e5-large"å’Œ"gte-large"çš„ `pooler`é‡‡ç”¨`mean`ï¼Œå…¶ä»–æ¨¡å‹çš„`pooler`é‡‡ç”¨`cls`.
  - "jina-embeddings-v2-base-en"æ¨¡å‹åœ¨è½½å…¥æ—¶éœ€è¦`trust_remote_code`ã€‚

#### 2. Reranker Models

Run following cmd to evaluate `your_reranker_model` (e.g. "maidalun1020/bce-reranker-base_v1") in **bilingual and crosslingual settings** (e.g. `["en", "zh", "en-zh", "zh-en"]`).

  è¿è¡Œä¸‹é¢å‘½ä»¤è¯„æµ‹`your_reranker_model`ï¼ˆæ¯”å¦‚ï¼Œ`maidalun1020/bce-reranker-base_v1`ï¼‰ã€‚è¯„æµ‹ä»»åŠ¡å°†ä¼šåœ¨ **åŒè¯­ç§å’Œè·¨è¯­ç§**ï¼ˆæ¯”å¦‚ï¼Œ`["en", "zh", "en-zh", "zh-en"]`ï¼‰æ¨¡å¼ä¸‹è¯„æµ‹ï¼š

```bash
python BCEmbedding/tools/eval_mteb/eval_reranker_mteb.py --model_name_or_path maidalun1020/bce-reranker-base_v1
```

The evaluation tasks contain ***12 datastes*** of **"Reranking"**.

  è¯„æµ‹åŒ…å« **"Reranking"** ä»»åŠ¡çš„ ***12ä¸ªæ•°æ®é›†***ã€‚

#### 3. Metrics Visualization Tool

We proveide a one-click script to sumarize evaluation results of `embedding` and `reranker` models as [Embedding Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md) and [Reranker Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md).

  æˆ‘ä»¬æä¾›äº†`embedding`å’Œ`reranker`æ¨¡å‹çš„æŒ‡æ ‡å¯è§†åŒ–ä¸€é”®è„šæœ¬ï¼Œè¾“å‡ºä¸€ä¸ªmarkdownæ–‡ä»¶ï¼Œè¯¦è§[Embeddingæ¨¡å‹æŒ‡æ ‡æ±‡æ€»](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md)å’Œ[Rerankeræ¨¡å‹æŒ‡æ ‡æ±‡æ€»](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md)ã€‚

```bash
python BCEmbedding/evaluation/mteb/summarize_eval_results.py --results_dir {your_embedding_results_dir | your_reranker_results_dir}
```

### Evaluate RAG by LlamaIndex

[LlamaIndex](https://github.com/run-llama/llama_index) is a famous data framework for LLM-based applications, particularly in RAG. Recently, the [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) has evaluated the popular embedding and reranker models in RAG pipeline and attract great attention. Now, we follow its pipeline to evaluate our `BCEmbedding`.

  [LlamaIndex](https://github.com/run-llama/llama_index)æ˜¯ä¸€ä¸ªè‘—åçš„å¤§æ¨¡å‹åº”ç”¨çš„å¼€æºå·¥å…·ï¼Œåœ¨RAGä¸­å¾ˆå—æ¬¢è¿ã€‚æœ€è¿‘ï¼Œ[LlamaIndexåšå®¢](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)å¯¹å¸‚é¢ä¸Šå¸¸ç”¨çš„embeddingå’Œrerankeræ¨¡å‹è¿›è¡ŒRAGæµç¨‹çš„è¯„æµ‹ï¼Œå¸å¼•å¹¿æ³›å…³æ³¨ã€‚ä¸‹é¢æˆ‘ä»¬æŒ‰ç…§è¯¥è¯„æµ‹æµç¨‹éªŒè¯`BCEmbedding`åœ¨RAGä¸­çš„æ•ˆæœã€‚

First, install LlamaIndex:
```bash
pip install llama-index==0.9.22
```

#### 1. Metrics Definition

- Hit Rate:

  Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it's about how often our system gets it right within the top few guesses. ***The larger, the better.***

- Mean Reciprocal Rank (MRR):
  
  For each query, MRR evaluates the system's accuracy by looking at the rank of the highest-placed relevant document. Specifically, it's the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it's second, the reciprocal rank is 1/2, and so on. ***The larger, the better.***

  - å‘½ä¸­ç‡ï¼ˆHit Rateï¼‰
  
    å‘½ä¸­ç‡è®¡ç®—çš„æ˜¯åœ¨æ£€ç´¢çš„å‰kä¸ªæ–‡æ¡£ä¸­æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æŸ¥è¯¢æ‰€å çš„æ¯”ä¾‹ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒåæ˜ äº†æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å‰å‡ æ¬¡çŒœæµ‹ä¸­ç­”å¯¹çš„é¢‘ç‡ã€‚***è¯¥æŒ‡æ ‡è¶Šå¤§è¶Šå¥½ã€‚***
  
  - å¹³å‡å€’æ•°æ’åï¼ˆMean Reciprocal Rankï¼ŒMRRï¼‰
    
    å¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼ŒMRRé€šè¿‡æŸ¥çœ‹æœ€é«˜æ’åçš„ç›¸å…³æ–‡æ¡£çš„æ’åæ¥è¯„ä¼°ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ˜¯åœ¨æ‰€æœ‰æŸ¥è¯¢ä¸­è¿™äº›æ’åçš„å€’æ•°çš„å¹³å‡å€¼ã€‚å› æ­¤ï¼Œå¦‚æœç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£æ˜¯æ’åæœ€é å‰çš„ç»“æœï¼Œå€’æ•°æ’åå°±æ˜¯1ï¼›å¦‚æœæ˜¯ç¬¬äºŒä¸ªï¼Œå€’æ•°æ’åå°±æ˜¯1/2ï¼Œä¾æ­¤ç±»æ¨ã€‚***è¯¥æŒ‡æ ‡è¶Šå¤§è¶Šå¥½ã€‚***

#### 2. Reproduce [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)

In order to compare our `BCEmbedding` with other embedding and reranker models fairly, we provide a one-click script to reproduce results of the LlamaIndex Blog, including our `BCEmbedding`:

  ä¸ºäº†å…¬å¹³èµ·è§ï¼Œè¿è¡Œä¸‹é¢è„šæœ¬ï¼Œå¤ç°LlamaIndexåšå®¢çš„ç»“æœï¼Œå°†`BCEmbedding`ä¸å…¶ä»–embeddingå’Œrerankeræ¨¡å‹è¿›è¡Œå¯¹æ¯”åˆ†æï¼š

```bash
# There should be two GPUs available at least.
CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_reproduce.py
```

Then, sumarize the evaluation results by:
```bash
python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir results/rag_reproduce_results
```

Results Reproduced from the LlamaIndex Blog can be checked in ***[Reproduced Summary of RAG Evaluation](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***, with some obvious ***conclusions***:
- In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.
- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.
- ***The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA.***

  è¾“å‡ºçš„æŒ‡æ ‡æ±‡æ€»è¯¦è§ ***[LlamaIndex RAGè¯„æµ‹ç»“æœå¤ç°](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***ã€‚ä»è¯¥å¤ç°ç»“æœä¸­ï¼Œå¯ä»¥çœ‹å‡ºï¼š
  - åœ¨`WithoutReranker`è®¾ç½®ä¸‹ï¼ˆ**ç«–æ’å¯¹æ¯”**ï¼‰ï¼Œ`bce-embedding-base_v1`æ¯”å…¶ä»–embeddingæ¨¡å‹æ•ˆæœéƒ½è¦å¥½ã€‚
  - åœ¨å›ºå®šembeddingæ¨¡å‹è®¾ç½®ä¸‹ï¼Œå¯¹æ¯”ä¸åŒrerankeræ•ˆæœï¼ˆ**æ¨ªæ’å¯¹æ¯”**ï¼‰ï¼Œ`bce-reranker-base_v1`æ¯”å…¶ä»–rerankeræ¨¡å‹æ•ˆæœéƒ½è¦å¥½ã€‚
  - ***`bce-embedding-base_v1`å’Œ`bce-reranker-base_v1`ç»„åˆï¼Œè¡¨ç°SOTAã€‚***

#### 3. Broad Domain Adaptability

The evaluation of [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) is **monolingual, small amount of data, and specific domain** (just including "llama2" paper). In order to evaluate the **broad domain adaptability, bilingual and crosslingual capability**, we follow the blog to build a multiple domains evaluation dataset (includding "Computer Science", "Physics", "Biology", "Economics", "Math", and "Quantitative Finance"), named [CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset), **by OpenAI `gpt-4-1106-preview` for high quality**.

  åœ¨ä¸Šè¿°çš„[LlamaIndexåšå®¢](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)çš„è¯„æµ‹æ•°æ®åªç”¨äº†â€œllama2â€è¿™ä¸€ç¯‡æ–‡ç« ï¼Œè¯¥è¯„æµ‹æ˜¯ **å•è¯­ç§ï¼Œå°æ•°æ®é‡ï¼Œç‰¹å®šé¢†åŸŸ** çš„ã€‚ä¸ºäº†å…¼å®¹æ›´çœŸå®æ›´å¹¿çš„ç”¨æˆ·ä½¿ç”¨åœºæ™¯ï¼Œè¯„æµ‹ç®—æ³•æ¨¡å‹çš„ **é¢†åŸŸæ³›åŒ–æ€§ï¼ŒåŒè¯­å’Œè·¨è¯­ç§èƒ½åŠ›**ï¼Œæˆ‘ä»¬æŒ‰ç…§è¯¥åšå®¢çš„æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªå¤šé¢†åŸŸï¼ˆè®¡ç®—æœºç§‘å­¦ï¼Œç‰©ç†å­¦ï¼Œç”Ÿç‰©å­¦ï¼Œç»æµå­¦ï¼Œæ•°å­¦ï¼Œé‡åŒ–é‡‘èç­‰ï¼‰çš„åŒè¯­ç§ã€è·¨è¯­ç§è¯„æµ‹æ•°æ®ï¼Œ[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)ã€‚**ä¸ºäº†ä¿è¯æ„å»ºæ•°æ®çš„é«˜è´¨é‡ï¼Œæˆ‘ä»¬é‡‡ç”¨OpenAIçš„`gpt-4-1106-preview`ã€‚**

First, run following cmd to evaluate the most popular and powerful embedding and reranker models:

```bash
# There should be two GPUs available at least.
CUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_multiple_domains.py
```

Then, run the following script to sumarize the evaluation results:
```bash
python BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir results/rag_results
```

The summary of multiple domains evaluations can be seen in <a href=#1-multiple-domains-scenarios>Multiple Domains Scenarios</a>.

## ğŸ“ˆ Leaderboard

### Semantic Representation Evaluations in MTEB

#### 1. Embedding Models

| Model | Dimensions | Pooler | Instructions | Retrieval (47) | STS (19) | PairClassification (5) | Classification (21) | Reranking (12) | Clustering (15) | ***AVG*** (119) |  
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  
| bge-base-en-v1.5 | 768 | `cls` | Need | 37.14 | 55.06 | 75.45 | 59.73 | 43.00 | 37.74 | 47.19 |  
| bge-base-zh-v1.5 | 768 | `cls` | Need | 47.63 | 63.72 | 77.40 | 63.38 | 54.95 | 32.56 | 53.62 |  
| bge-large-en-v1.5 | 1024 | `cls` | Need | 37.18 | 54.09 | 75.00 | 59.24 | 42.47 | 37.32 | 46.80 |  
| bge-large-zh-v1.5 | 1024 | `cls` | Need | 47.58 | 64.73 | 79.14 | 64.19 | 55.98 | 33.26 | 54.23 |  
| e5-large-v2 | 1024 | `mean` | Need | 35.98 | 55.23 | 75.28 | 59.53 | 42.12 | 36.51 | 46.52 |  
| gte-large | 1024 | `mean` | Free | 36.68 | 55.22 | 74.29 | 57.73 | 42.44 | 38.51 | 46.67 |  
| gte-large-zh | 1024 | `cls` | Free | 41.15 | 64.62 | 77.58 | 62.04 | 55.62 | 33.03 | 51.51 |  
| jina-embeddings-v2-base-en | 768 | `mean` | Free | 31.58 | 54.28 | 74.84 | 58.42 | 41.16 | 34.67 | 44.29 |  
| m3e-base | 768 | `mean` | Free | 46.29 | 63.93 | 71.84 | 64.08 | 52.38 | 37.84 | 53.54 |  
| m3e-large | 1024 | `mean` | Free | 34.85 | 59.74 | 67.69 | 60.07 | 48.99 | 31.62 | 46.78 |  
| multilingual-e5-base | 768 | `mean` | Need | 54.73 | 65.49 | 76.97 | 69.72 | 55.01 | 38.44 | 58.34 |  
| multilingual-e5-large | 1024 | `mean` | Need | 56.76 | 66.79 | 78.80 | 71.61 | 56.49 | 43.09 | 60.50 |  
| ***bce-embedding-base_v1*** | 768 | `cls` | Free | 57.60 | 65.73 | 74.96 | 69.00 | 57.29 | 38.95 | 59.43 |  

***NOTE:***
- Our ***bce-embedding-base_v1*** outperforms other opensource embedding models with comparable model size.
- ***114 datastes*** of **"Retrieval", "STS", "PairClassification", "Classification", "Reranking" and "Clustering"** in `["en", "zh", "en-zh", "zh-en"]` setting.
- The [crosslingual evaluation datasets](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py) we released belong to `Retrieval` task.
- More evaluation details please check [Embedding Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md).

  ***è¦ç‚¹ï¼š***
  - å¯¹æ¯”å…¶ä»–å¼€æºçš„ç›¸åŒè§„æ¨¡çš„embeddingæ¨¡å‹ï¼Œ***bce-embedding-base_v1*** è¡¨ç°æœ€å¥½ï¼Œæ•ˆæœæ¯”æœ€å¥½çš„largeæ¨¡å‹ç¨å·®ã€‚
  - è¯„æµ‹åŒ…å« **"Retrieval"ï¼Œ "STS"ï¼Œ "PairClassification"ï¼Œ "Classification"ï¼Œ "Reranking"å’Œ"Clustering"** è¿™å…­å¤§ç±»ä»»åŠ¡çš„å…± ***114ä¸ªæ•°æ®é›†***ã€‚
  - æˆ‘ä»¬å¼€æºçš„[è·¨è¯­ç§è¯­ä¹‰è¡¨å¾è¯„æµ‹æ•°æ®](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)å±äº`Retrieval`ä»»åŠ¡ã€‚
  - æ›´è¯¦ç»†çš„è¯„æµ‹ç»“æœè¯¦è§[Embeddingæ¨¡å‹æŒ‡æ ‡æ±‡æ€»](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md)ã€‚

#### 2. Reranker Models

| Model                              | Reranking (12) | ***AVG*** (12) |
| :--------------------------------- | :-------------: | :--------------------: |
| bge-reranker-base                  |      59.04      |         59.04         |
| bge-reranker-large                 |      60.86      |         60.86         |
| ***bce-reranker-base_v1*** | **61.29** |  ***61.29***  |

***NOTE:***
- Our ***bce-reranker-base_v1*** outperforms other opensource reranker models.
- ***12 datastes*** of **"Reranking"** in `["en", "zh", "en-zh", "zh-en"]` setting.
- More evaluation details please check [Reranker Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md).

  ***è¦ç‚¹ï¼š***
  - ***bce-reranker-base_v1*** ä¼˜äºå…¶ä»–å¼€æºrerankeræ¨¡å‹ã€‚
  - è¯„æµ‹åŒ…å« **"Reranking"** ä»»åŠ¡çš„ ***12ä¸ªæ•°æ®é›†***ã€‚
  - æ›´è¯¦ç»†çš„è¯„æµ‹ç»“æœè¯¦è§[Rerankeræ¨¡å‹æŒ‡æ ‡æ±‡æ€»](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md)

### RAG Evaluations in LlamaIndex

#### 1. Multiple Domains Scenarios

![image/jpeg](assets/rag_eval_multiple_domains_summary.jpg)

***NOTE:***
- Evaluated in **`["en", "zh", "en-zh", "zh-en"]` setting**.
- In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.
- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.
- **The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA**.

  ***è¦ç‚¹ï¼š***
  - è¯„æµ‹æ˜¯åœ¨`["en", "zh", "en-zh", "zh-en"]`è®¾ç½®ä¸‹ã€‚
  - åœ¨`WithoutReranker`è®¾ç½®ä¸‹ï¼ˆ**ç«–æ’å¯¹æ¯”**ï¼‰ï¼Œ`bce-embedding-base_v1`ä¼˜äºå…¶ä»–Embeddingæ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºã€‚
  - åœ¨å›ºå®šEmbeddingæ¨¡å‹è®¾ç½®ä¸‹ï¼Œå¯¹æ¯”ä¸åŒrerankeræ•ˆæœï¼ˆ**æ¨ªæ’å¯¹æ¯”**ï¼‰ï¼Œ`bce-reranker-base_v1`æ¯”å…¶ä»–rerankeræ¨¡å‹æ•ˆæœéƒ½è¦å¥½ï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºã€‚
  - ***`bce-embedding-base_v1`å’Œ`bce-reranker-base_v1`ç»„åˆï¼Œè¡¨ç°SOTAã€‚***

## ğŸ›  Youdao's BCEmbedding API

For users who prefer a hassle-free experience without the need to download and configure the model on their own systems, `BCEmbedding` is readily accessible through Youdao's API. This option offers a streamlined and efficient way to integrate BCEmbedding into your projects, bypassing the complexities of manual setup and maintenance. Detailed instructions and comprehensive API documentation are available at [Youdao BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html). Here, you'll find all the necessary guidance to easily implement `BCEmbedding` across a variety of use cases, ensuring a smooth and effective integration for optimal results.

  å¯¹äºé‚£äº›æ›´å–œæ¬¢ç›´æ¥è°ƒç”¨apiçš„ç”¨æˆ·ï¼Œæœ‰é“æä¾›æ–¹ä¾¿çš„`BCEmbedding`è°ƒç”¨apiã€‚è¯¥æ–¹å¼æ˜¯ä¸€ç§ç®€åŒ–å’Œé«˜æ•ˆçš„æ–¹å¼ï¼Œå°†`BCEmbedding`é›†æˆåˆ°æ‚¨çš„é¡¹ç›®ä¸­ï¼Œé¿å¼€äº†æ‰‹åŠ¨è®¾ç½®å’Œç³»ç»Ÿç»´æŠ¤çš„å¤æ‚æ€§ã€‚æ›´è¯¦ç»†çš„apiè°ƒç”¨æ¥å£è¯´æ˜è¯¦è§[æœ‰é“BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html)ã€‚

## ğŸ§² WeChat Group

Welcome to scan the QR code below and join the WeChat group.

  æ¬¢è¿å¤§å®¶æ‰«ç åŠ å…¥å®˜æ–¹å¾®ä¿¡äº¤æµç¾¤ã€‚

![image/jpeg](assets/Wechat.jpg)

## âœï¸ Citation

If you use `BCEmbedding` in your research or project, please feel free to cite and star it:

  å¦‚æœåœ¨æ‚¨çš„ç ”ç©¶æˆ–ä»»ä½•é¡¹ç›®ä¸­ä½¿ç”¨æœ¬å·¥ä½œï¼Œçƒ¦è¯·æŒ‰ç…§ä¸‹æ–¹è¿›è¡Œå¼•ç”¨ï¼Œå¹¶æ‰“ä¸ªå°æ˜Ÿæ˜Ÿï½

```
@misc{youdao_bcembedding_2023,
    title={BCEmbedding: Bilingual and Crosslingual Embedding for RAG},
    author={NetEase Youdao, Inc.},
    year={2023},
    howpublished={\url{https://github.com/netease-youdao/BCEmbedding}}
}
```

## ğŸ” License

`BCEmbedding` is licensed under [Apache 2.0 License](https://github.com/netease-youdao/BCEmbedding/blob/master/LICENSE)

## ğŸ”— Related Links

[Netease Youdao - QAnything](https://github.com/netease-youdao/qanything)

[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

[MTEB](https://github.com/embeddings-benchmark/mteb)

[C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)

[LLama Index](https://github.com/run-llama/llama_index) | [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)