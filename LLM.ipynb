{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fde7c0-ecc0-4eb7-953f-e7ecd89b09c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install BCEmbedding==0.1.1\n",
    "# 安装 streamlit\n",
    "# ! pip install streamlit==1.24.0\n",
    "# !pip install langchain\n",
    "# !pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c6c91d-e353-45e2-8279-4cb205a900d9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:07:15.150022Z",
     "iopub.status.busy": "2024-08-21T23:07:15.149610Z",
     "iopub.status.idle": "2024-08-21T23:07:15.156003Z",
     "shell.execute_reply": "2024-08-21T23:07:15.155515Z",
     "shell.execute_reply.started": "2024-08-21T23:07:15.149998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import BCEmbedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "188eb838-007e-4b82-a5dd-fdce51ab566e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:28:48.736681Z",
     "iopub.status.busy": "2024-08-21T23:28:48.736316Z",
     "iopub.status.idle": "2024-08-21T23:28:48.865481Z",
     "shell.execute_reply": "2024-08-21T23:28:48.864681Z",
     "shell.execute_reply.started": "2024-08-21T23:28:48.736658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#用新模型的向量模型类\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name: str, device: str = 'cuda'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_embeddings(self, sentences: List[str], batch_size: int = 8) -> np.ndarray:  \n",
    "        all_embeddings = []  \n",
    "        for i in range(0, len(sentences), batch_size):  \n",
    "            batch = sentences[i:i + batch_size]  \n",
    "            #print(batch)\n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")  \n",
    "            #print(inputs)\n",
    "            inputs_on_device = {k: v.to(self.device) for k, v in inputs.items()}  \n",
    "            with torch.no_grad():  \n",
    "                outputs = self.model(**inputs_on_device, return_dict=True)  \n",
    "            embeddings = outputs.last_hidden_state[:, 0]  \n",
    "            embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  \n",
    "            all_embeddings.append(embeddings.cpu().numpy())  \n",
    "        return np.vstack(all_embeddings)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4850e61-9d10-4551-9d4c-813b9ad3cfd4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:31:09.244336Z",
     "iopub.status.busy": "2024-08-21T23:31:09.243973Z",
     "iopub.status.idle": "2024-08-21T23:31:09.251037Z",
     "shell.execute_reply": "2024-08-21T23:31:09.250549Z",
     "shell.execute_reply.started": "2024-08-21T23:31:09.244307Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义向量库索引类\n",
    "class VectorStoreIndex:\n",
    "    \"\"\"\n",
    "    class for VectorStoreIndex\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doecment_path: str, embed_model: EmbeddingModel) -> None:\n",
    "        self.documents = []\n",
    "        for line in open(doecment_path, 'r', encoding='utf-8'):\n",
    "            line = line.strip()\n",
    "            self.documents.append(line)\n",
    "\n",
    "        self.embed_model = embed_model\n",
    "        self.vectors = self.embed_model.get_embeddings(self.documents)\n",
    "\n",
    "        print(f'Loading {len(self.documents)} documents for {doecment_path}.')\n",
    "\n",
    "    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        calculate cosine similarity between two vectors\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product / magnitude\n",
    "\n",
    "    def query(self, question: str, k: int = 2) -> List[str]:\n",
    "        question_vector = self.embed_model.get_embeddings([question])[0]\n",
    "        result = np.array([self.get_similarity(question_vector, vector) for vector in self.vectors])\n",
    "        return np.array(self.documents)[result.argsort()[-k:][::-1]].tolist()\n",
    "    '''\n",
    "    def web_search(self, search_list):\n",
    "        os.environ[\"SERPER_API_KEY\"] = \"88a8892a02409063f02a3bb97ac08b36fb213ae7\"\n",
    "        search = GoogleSerperAPIWrapper()\n",
    "        search_result = ''\n",
    "        for prof_name in search_list:\n",
    "            search_item = prof_name + \"research interest\"\n",
    "            search_result+= str(search.run(search_item)) + '\\n'\n",
    "            \n",
    "        return search_result\n",
    "            # results = search.results(search_item)\n",
    "            # pprint.pp(results)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f768e1c4-cd27-4287-bd0f-19954e2434a9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:43:59.420918Z",
     "iopub.status.busy": "2024-08-21T23:43:59.420572Z",
     "iopub.status.idle": "2024-08-21T23:44:00.150899Z",
     "shell.execute_reply": "2024-08-21T23:44:00.150326Z",
     "shell.execute_reply.started": "2024-08-21T23:43:59.420895Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create embedding model...\n"
     ]
    }
   ],
   "source": [
    "print(\"> Create embedding model...\")\n",
    "embed_model = EmbeddingModel('AI-ModelScope/BCEmbeddingmodel')\n",
    "# embed_model.get_embeddings(sentences)\n",
    "# # init embedding model\n",
    "# model = EmbeddingModel(model_name_or_path=\"AI-ModelScope/BCEmbeddingmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b278205a-cf45-4b34-a38e-db48a9a49f7f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:55:53.729105Z",
     "iopub.status.busy": "2024-08-21T23:55:53.728605Z",
     "iopub.status.idle": "2024-08-21T23:55:53.759805Z",
     "shell.execute_reply": "2024-08-21T23:55:53.759285Z",
     "shell.execute_reply.started": "2024-08-21T23:55:53.729073Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create index...\n",
      "Loading 10 documents for ./test.txt.\n",
      "(10, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"> Create index...\")\n",
    "doecment_path = './test.txt'\n",
    "index = VectorStoreIndex(doecment_path, embed_model)\n",
    "\n",
    "#查看向量库的shape\n",
    "_vector = np.array(index.vectors)\n",
    "print(_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1616d89e-a3c0-4d67-b5ae-3a4523fcbcbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T23:07:56.314027Z",
     "iopub.status.busy": "2024-08-21T23:07:56.313575Z",
     "iopub.status.idle": "2024-08-21T23:07:56.612723Z",
     "shell.execute_reply": "2024-08-21T23:07:56.612189Z",
     "shell.execute_reply.started": "2024-08-21T23:07:56.314001Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "712f36f2-73fb-44ff-9050-e8e89e15ae2f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:56:00.707613Z",
     "iopub.status.busy": "2024-08-21T23:56:00.707020Z",
     "iopub.status.idle": "2024-08-21T23:56:00.719595Z",
     "shell.execute_reply": "2024-08-21T23:56:00.719077Z",
     "shell.execute_reply.started": "2024-08-21T23:56:00.707588Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Question: Recommend university in security\n",
      "> Context: ['The University of Maryland ranks 15th overall in computer science, with an AI program ranked 18th and strong interdisciplinary research ranked 8th. The university’s systems program is ranked 9th, excelling in areas such as cybersecurity, cloud computing, and networking. Its theory program is ranked 16th, showcasing its strengths in algorithms and computational complexity. Located near Washington, D.C., the University of Maryland offers students access to government agencies, research labs, and top tech companies in the region.', 'Cornell University ranks 7th overall in computer science, excelling in theory (ranked 6th) and AI (ranked 7th). The university’s interdisciplinary program is ranked 12th, with collaborations spanning fields like computational biology, social sciences, and economics. In systems, Cornell is ranked 19th, with strengths in computer architecture, distributed systems, and databases. Located in Ithaca, New York, Cornell provides students with a strong academic foundation and ample research opportunities across a wide range of fields.']\n"
     ]
    }
   ],
   "source": [
    "question = 'Recommend university in security'\n",
    "print('> Question:', question)\n",
    "\n",
    "context = index.query(question)\n",
    "print('> Context:', context)\n",
    "\n",
    "#context_web = index.web_search(context)\n",
    "#print('> Context_web:', context_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e8b3219-cca5-4144-bbd6-6412cc3c5ca0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:56:38.544042Z",
     "iopub.status.busy": "2024-08-21T23:56:38.543542Z",
     "iopub.status.idle": "2024-08-21T23:56:38.549934Z",
     "shell.execute_reply": "2024-08-21T23:56:38.549381Z",
     "shell.execute_reply.started": "2024-08-21T23:56:38.544009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义大语言模型类\n",
    "class LLM:\n",
    "    \"\"\"\n",
    "    class for Yuan2.0 LLM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str) -> None:\n",
    "        print(\"Creat tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n",
    "\n",
    "        print(\"Creat model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "\n",
    "        print(f'Loading Yuan2.0 model from {model_path}.')\n",
    "\n",
    "    def generate(self, question: str, context: List):#, context_web: str):\n",
    "        if context:\n",
    "            prompt = f'背景：{context}\\n 问题：{question}\\n 我现在正在申请计算机领域的博士，以上背景是关于学校和教授的学术信息，请根据以上信息回答我的问题'\n",
    "        else:\n",
    "            prompt = question\n",
    "\n",
    "        prompt += \"<sep>\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        outputs = self.model.generate(inputs, do_sample=False, max_length=2048)\n",
    "        output = self.tokenizer.decode(outputs[0])\n",
    "\n",
    "        print(output.split(\"<sep>\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bbfc0659-14cf-4622-b1bb-461a151d601a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-21T23:56:40.529368Z",
     "iopub.status.busy": "2024-08-21T23:56:40.528872Z",
     "iopub.status.idle": "2024-08-21T23:56:50.971469Z",
     "shell.execute_reply": "2024-08-21T23:56:50.970916Z",
     "shell.execute_reply.started": "2024-08-21T23:56:40.529338Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create Yuan2.0 LLM...\n",
      "Creat tokenizer...\n",
      "Creat model...\n",
      "Loading Yuan2.0 model from ./IEITYuan/Yuan2-2B-Mars-hf.\n",
      "> Without RAG:\n",
      " 1. 联合国安全理事会第1267(1999)号决议<eod>\n",
      "> With RAG:\n",
      " 根据以上信息，我建议你考虑以下几个方面来选择合适的学校：\n",
      "1. 学校的计算机科学排名：根据学校的计算机科学排名，选择一个在该领域内排名较高的学校。你可以参考学校的整体排名、学科排名和师资力量等指标。\n",
      "2. 学校的教授团队：了解学校的教授团队，特别是那些在计算机领域有丰富经验和专业知识的教授。他们的教学和研究水平对你的学习和职业发展都非常重要。\n",
      "3. 学校的科研实力：考虑学校的科研实力，包括是否有科研项目资助、实验室设备和资源等。这些因素将直接影响到你在计算机领域的研究能力和发展潜力。\n",
      "4. 学校的就业和实习机会：了解学校的就业和实习机会，尤其是与计算机科学相关的实习和就业机会。一个好的学校可以提供更多的实践机会和职业发展空间。\n",
      "综上所述，选择一个合适的学校需要综合考虑学校的计算机科学排名、教授团队、科研实力和就业机会等因素。建议你仔细研究学校的官方网站和相关排名，与学校的招生办公室或教授进行交流，以便做出更准确和明智的选择。<eod>\n"
     ]
    }
   ],
   "source": [
    "print(\"> Create Yuan2.0 LLM...\")\n",
    "model_path = './IEITYuan/Yuan2-2B-Mars-hf'\n",
    "#model_path = './IEITYuan/Yuan2-2B-July-hf'\n",
    "llm = LLM(model_path)\n",
    "print('> Without RAG:')\n",
    "llm.generate(question, [])\n",
    "# llm.generate(question, [],'')\n",
    "\n",
    "print('> With RAG:')\n",
    "# llm.generate(question, context)\n",
    "llm.generate(question, context)#, context_web)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
